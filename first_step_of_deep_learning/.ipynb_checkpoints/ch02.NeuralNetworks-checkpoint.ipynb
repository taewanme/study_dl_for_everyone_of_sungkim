{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장. 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 서론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주제: 인공 신경망, Neural Network\n",
    "  - 머신러닝 $\\rightarrow$ 학습규칙\n",
    "  - 모델 $\\rightarrow$ 신경망\n",
    "  - ![](./images/02.nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2장은 단층 신경망을 위주로 학습\n",
    "  - 다층 신경망은 3장에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2장 구성\n",
    "- 2.2: 노드란?\n",
    "- 2.3: 노드의 신호의 전달 과정\n",
    "- 2.4: 신경망에 지도학습 적용\n",
    "- 2.5, 2.6: 신경망의 학습 과정\n",
    "- 2.7: 신경망 학습에 학습 데이터 운영 방식\n",
    "- 2.8: 단층 신경망 구현\n",
    "- 2.9: 단층 신경망의 한계\n",
    "- 2.10: 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 신경망 노드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 뇌가 데이터를 저장하는 방식\n",
    "  - 연결관계를 바꾸는 방식으로 저장\n",
    "  - 별도의 저장 매체가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|뇌|신경망|\n",
    "|---|---|\n",
    "|신경세포|뉴런|\n",
    "|신경세포 연결|연결가중치|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3개의 입력을 받는 노드\n",
    "  - x: 입력 정보, input\n",
    "  - w: 가중치, weight\n",
    "  - b: 편향, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.02.neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 외부에서 입력된 데이터는 가중치의 곱의 합으로 전달\n",
    "- 가중치는 입력 데이터를 중요도를 결정\n",
    "- Weighted sum\n",
    "  - $v=w_1*x_1+w_2*x_2+w_3*x_3+b$\n",
    "  - $v = WX + b$\n",
    "  - W = $[w_1,w_2,w_3]$\n",
    "  - X = $[x_1, w_2$, w_3$]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성함수 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y = \\varphi(v)$\n",
    "  - $\\varphi(\\cdot)$은 활성함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 노드 동작 방식 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Weighted Sum\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v & = w_1*x_1+w_2*x_2+w_3*x_3+b \\\\\n",
    "& = WX+b \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 가중치로 외부 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\varphi(v)=\\varphi(WX+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 신경망의 계층 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력층: 들어온 신호를 그대로 다음 노드에 전달\n",
    "  - Weight Sum이나 Activation Function을 처리하지 않음\n",
    "- 출력층\n",
    "- 은닉층:입력층과 출력층 사이의 층 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.03.mpl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단층신경망: 입력층과 출력층으로만 구성\n",
    "- 다층신경망: 입력층과 출력층 사이에 은닉충이 존재\n",
    "- 심층신경망: 은닉층이 2개이상인 다층신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단층 신경망\n",
    "  - ![](./images/02.04.spl.png)\n",
    "- 다층 신경망\n",
    "  - ![](./images/02.05.dpl.png)\n",
    "- 심층 신경망\n",
    "  - ![](./images/02.06.mpl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.07.nn-ex01.png)\n",
    "- activation function\n",
    "  - $\\varphi(x) = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example01 - phase 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.08.nn-ex01-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### solution 1 by math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  특정 노드에 적용되는 가중치를 컬럼 벡터로 만들어야 함\n",
    "  - 편의상 입력 노드의 모든 가중치를 row 벡터로 등록하고\n",
    "  - transpose함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.09.nn-ex01-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### solution 1 by numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_input [[3 2]\n",
      " [1 4]]\n",
      "h1_input [ 6 11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation_func(x):\n",
    "    return x\n",
    "\n",
    "input = np.array([1, 2])\n",
    "w_01 = np.array([[3, 2],[1,4]])\n",
    "bias = 1\n",
    "\n",
    "v = np.dot(w_01.T,input)+bias\n",
    "h1_input = activation_func(v)\n",
    "print(\"W_input\", w_01)\n",
    "print(\"h1_input\", h1_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example01 - phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.10.nn-ex01-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### solution 1 by math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.10.nn-ex01-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_input [[3 5]\n",
      " [2 1]]\n",
      "h1_input [41 42]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation_func(x):\n",
    "    return x\n",
    "\n",
    "h1_input = np.array([6, 11])\n",
    "w_02 = np.array([[3, 5],[2,1]])\n",
    "bias = 1\n",
    "\n",
    "v = np.dot(w_02.T,h1_input)+bias\n",
    "out = activation_func(v)\n",
    "print(\"W_input\", w_02)\n",
    "print(\"h1_input\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성함수로 선형삼수를 사용하지 말것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 은닉층을 추가하는 효과를 상실\n",
    "- ![](./images/02.10.nn-ex01-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활성함수를 선형함수로 사용할 경우 다음 네트워크는 동치\n",
    "- 활성함수가 선형함수이면 hidden layer의 효과가 없음\n",
    "- 활성함수는 출력노드에서는 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.12.nn-ex01-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w*x+b, w.T*b, x*w, ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.27.shape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 신경망의 지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 지도학습 실행 절차\n",
    "1. 가중치를 적절히 초기화\n",
    "1. 데이터 중 입력 값을 확보하여 출력값과 정답과 비교하여 오차를 계산\n",
    "1. 오차를 줄어들도록 신경망의 가중치를 조절\n",
    "1. 전체 학습에서 2~3단계를 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 단층 신경망 학습: 델타 규칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망은 정보를 가중치 형태로 저장\n",
    "- 새로운 정보를 학습시키면 가중치를 변경해야 함\n",
    "- <font color='red'>__학습규칙__</font>\n",
    "  - 정보에 맞춰 가중치를 체계적으로 변경하는 방법\n",
    "  - 학습 규칙의 대표적인 사례\n",
    "    - 델타규칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.14_error_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 델타 규칙의 원칙\n",
    "  - 어떤 입력 노드가 출력 노드의 오차에 기여했다면\n",
    "  - 두 노드의 연결 가중치는 입력 노드($x_j$)의 출력과 출력 노도($y_i$)의 오차($e_i$)에 비례해 조절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 델타 규칙 공식\n",
    "\n",
    "- $w_{ij} \\rightarrow w_{ij} + \\alpha e_i x_j$\n",
    "  - $x_j$: 입력 노드 j의 출력 (j=1, 2, 3, 4)\n",
    "  - $e_i$: 출력 노드 i의 출력 (i=1, 2, 3)\n",
    "  - $w_{ij}$: 입력노드 j와 출력노드 i의 연결 가중치\n",
    "  - $\\alpha$: 학습룰 ($0<\\alpha <= 0$)\n",
    "    - 학습률이 크면: 수렴을 못함\n",
    "    - 학습률이 작으면: 학습 속도 저하"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 델타 규칙 적용\n",
    "  - $w_{11} \\leftarrow w_{11} + \\alpha e_1 x_1$\n",
    "  - $w_{12} \\leftarrow w_{12} + \\alpha e_2 x_2$\n",
    "  - $w_{13} \\leftarrow w_{13} + \\alpha e_3 x_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='red'>__$e_i x_j$__</font> 의미\n",
    "  - 전체 오차에서 입력데이터가 차지하는 비율을 의미\n",
    "  - 원칙적으로 입력데이터의 합으로 x_j르 나눠서 비율을 계산해야 함\n",
    "  - 공식의 단순화를 위해서 비율화를 포기\n",
    "  - 수식 단순화의 산물\n",
    "  - 학습률에는 큰 변화가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 델타 규칙을 적용한 신경망 학습 과정\n",
    "\n",
    "1. 신경망의 가중치를 적절한 값으로 초기화\n",
    "1. 학습데이터에서 정답(레이블, $d_j$) 획득\n",
    "1. 입력데이터로 부터 출력데이터 확보하고 오차 계산: $e_i=d_i-y_i$\n",
    "1. 델타 규칙에 따라 가중치 계산: $\\Delta w_{ij}=\\alpha e_i x_j$\n",
    "1. 신경망 업데이트\n",
    "  - $w_{ij} \\leftarrow w_{ij} + \\Delta w_{ij}$\n",
    "1. 2~5단계 반복\n",
    "\n",
    "----\n",
    "- 현재 델타 규칙은 Activation Function을 무시한 상태\n",
    "- Activation Function의 효율을 적용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 델타 규칙은 경사 하강법의 일종\n",
    "  - 임의의 초기값으로 부터 단계적으로 정답을 찾아가는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 델타 규칙의 일반 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 델타 규칙의 고도화\n",
    "  - Activation function을 적용 함\n",
    "  \n",
    "|Version 1|Version 2|\n",
    "|---|----|\n",
    "|$w_{ij} \\leftarrow w_{ij} + \\alpha e_i x_j$|$w_{ij} \\leftarrow w_{ij} + \\alpha \\delta_i x_j$|\n",
    "|activation function의 고려사항 없음<br/>선형 함수를 activation function으로 사용|$\\delta_i=\\varphi'(v_i)e_i $ <br/>activation function의 순간 변화율을 반영함|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $w_{ij} \\leftarrow w_{ij} + \\alpha \\delta_i x_j$\n",
    "- $\\delta_i=\\varphi'(v_i)e_i $\n",
    "- 설명\n",
    "  - $e_i$: 출력노드 i의 오차\n",
    "  - $v_i$: 출력노드 i의 가중합\n",
    "  - $\\varphi'$: 출력노드 i의 활성함수인 $\\varphi$의 도함수\n",
    "\n",
    "----\n",
    "- $\\varphi'(v_i)$은 weighted sum에 대한  activation function의 미분계수임\n",
    "- $\\varphi'(v_i)$을 오차의 곱 의미\n",
    "  - 오차에 weighted sum이 activation function에 대한 순간 변화율을 적용\n",
    "  - 오차에 활성함수의 순간 변화율을 적용하여 비율화 함\n",
    "\n",
    "----\n",
    "\n",
    "- $\\varphi'(x) = 1$인경우\n",
    " - $\\delta_i = e_i$\n",
    " \n",
    "$$\n",
    "\\begin{align}\n",
    "w_{ij} & \\leftarrow w_{ij} + \\alpha \\delta_i x_j \\\\\n",
    "& \\leftarrow w_{ij} + \\alpha \\varphi'(v_i) e_i x_j \\\\\n",
    "& \\leftarrow w_{ij} + \\alpha * 1 * e_i x_j \\\\\n",
    "& \\leftarrow w_{ij} + \\alpha e_i x_j \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시그모이드 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\varphi(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 구현 및 도식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEgIUrmLiIQglbuISAhSuYuIhKD/B/cO8Hkhpar7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48ac689f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x_data = np.arange(-5, 5, 0.2)\n",
    "y_data = sigmoid(x_data)\n",
    "\n",
    "plt.plot(x_data, y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigmoid 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 델타규칙 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\varphi'(x)=\\varphi(x)(1-\\varphi(x))$\n",
    "- $\\delta_i=\\varphi'(v_i)e_i$\n",
    "- $\\delta_i=\\varphi(v_i)(1-\\varphi(v_i))e_i$\n",
    "- $w_{ij} \\leftarrow w_{ij} + \\alpha \\varphi(v_i)(1-\\varphi(v_i))e_ix_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 델타규칙 유도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid 함수 미분\n",
    "\n",
    "![](./images/02.19_sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.20_loss_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.21_loss_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이론이 먼저인가 알고리즘이 먼저인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떤 입력 노드가 출력 노드의 오차에 기여했다면, 두 노드의 연결 가충치는 해당 <font color=\"red\">입력의 출력($x_j$)</font>과 <font color=\"red\">출력 노드의 오차($e_i$)</font>에 비례하여 조절한다.\n",
    "  - page: 50\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{ij} & \\leftarrow w_{ij} + \\Delta w_{ij} \\\\\n",
    "& \\leftarrow w_{ij} + \\alpha \\delta_i x_j \\\\\n",
    "& \\leftarrow w_{ij} + \\alpha \\varphi'(v_i) e_i x_j \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 SGD, Batch, Mini Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치를 갱신하는 방법 소개\n",
    "  - stochastic 경사 하강법\n",
    "  - batch\n",
    "  - mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 stochastic 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stochastic gradient descent, SDG\n",
    "- 데이터 1건 마다 weight 갱신\n",
    "- 단점: 신경망 성능의 변화폭이 매우 큼\n",
    "\n",
    "$$\\Delta w_{ij} = \\alpha \\delta_i x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 배치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 학습 데이터에 관한 오차와 가중치 델타를 구한 후\n",
    "- 평균값으로 가중치를 한번에 갱신\n",
    "- 데이터 학습후 가중치 업데이트는 1회 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Delta w_{ij} = \\frac{1}{N}\\sum_{k=1}^{N}{\\Delta w_{ij}(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\Delta w_{ij}(k)$ : K번째 가중치 델타\n",
    "- N: 총데이터 건수\n",
    "\n",
    "\n",
    "- 단점\n",
    "  - 데이터가 많을 경우 가중치 업데이트가 오래 걸림\n",
    "  - 가중치 갱신도 느림\n",
    "  - 전체적인 학습 시간이 오래 걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.3 미니배치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SDG + Batch\n",
    "- 데이터 100건\n",
    "  - 임의로 20건을 골라 배치 방식으로 학습시킴\n",
    "  - 전체 데이터를 학습하기 위해서는 5번의 mini-batch\n",
    "\n",
    "----\n",
    "\n",
    "- epoch\n",
    "  - 전체 데이터를 모두 학습시킨 횟수\n",
    "\n",
    "- epoch에 필요한 학습 횟수\n",
    "  - SDG: N번\n",
    "  - Batch: 1번\n",
    "  - Mini-Batch: $\\frac{N}{len(mini-batch)}$\n",
    "    - N=All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 예제: 델타규칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3개의 입력 노드와 하나의 출력 노드로 구성된 신경망\n",
    "\n",
    "![](./images/02.18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_set = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 0, 1, 1],\n",
    "    [1, 1, 1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\delta_i & = \\varphi(v_i)(1-\\varphi(v_i))e_i \\\\\n",
    "\\Delta W_{ij} & = \\alpha\\delta_ix_j \\\\\n",
    "w_ij & \\leftarrow w_ij + \\Delta w_{ij}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.1 SGD 방식 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.22_sdg_nn_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\delta_i & = \\varphi(v_i)(1-\\varphi(v_i))e_i \\\\\n",
    "\\Delta W_{ij} & = \\alpha\\delta_ix_j \\\\\n",
    "w_ij & \\leftarrow w_ij + \\Delta w_{ij}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDNN:\n",
    "    def __init__(self, input_node, output_node, \n",
    "                learning_rate, epoch):\n",
    "        self._input_node = input_node\n",
    "        self._output_node = output_node\n",
    "        self._learning_rate = learning_rate\n",
    "        self._epoch = epoch\n",
    "        self._w_input = np.random.uniform(size=(input_node, output_node))\n",
    "        pass\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def _d_sigmoid(self, x):\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "    def _delta(self, y, error):\n",
    "        return d_sigmoid(y)*error\n",
    "    \n",
    "    def train(self, input_data, targets):\n",
    "        for _ in range(self._epoch):\n",
    "            for input, target in zip(input_data, targets): \n",
    "                x = input.reshape([-1, 1])\n",
    "                h = np.dot(self._w_input.T, x) # W.T * X\n",
    "                y = self._sigmoid(h)           # output \n",
    "                e = target - y                      # error\n",
    "                delta_w = self._learning_rate*self._delta(y, e)*x\n",
    "                #print(delta_w)\n",
    "                self._w_input += delta_w\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, input):\n",
    "        x = np.array(input).reshape([-1,1])\n",
    "        h = np.dot(self._w_input.T, x) # W.T * X\n",
    "        y = self._sigmoid(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1454251]]\n",
      "[[ 0.09369826]]\n",
      "[[ 0.92657535]]\n",
      "[[ 0.88461581]]\n"
     ]
    }
   ],
   "source": [
    "## training data\n",
    "training_set = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 0, 1, 1],\n",
    "    [1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "input_data = training_set[:, :3]\n",
    "targets = training_set[:, [-1]]\n",
    "\n",
    "n = SGDNN(3, 1, 0.05, 1000)\n",
    "n.train(input_data, targets)\n",
    "\n",
    "print(n.predict([0,0,1]))\n",
    "print(n.predict([0,1,1]))\n",
    "print(n.predict([1,0,1]))\n",
    "print(n.predict([1,1,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.2 Batch 방식 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNN:\n",
    "    def __init__(self, input_node, output_node, \n",
    "                learning_rate, epoch):\n",
    "        self._input_node = input_node\n",
    "        self._output_node = output_node\n",
    "        self._learning_rate = learning_rate\n",
    "        self._epoch = epoch\n",
    "        self._w_input = np.random.uniform(size=(input_node, output_node))\n",
    "        pass\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def _d_sigmoid(self, x):\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "    def _delta(self, Y, E):\n",
    "        return d_sigmoid(Y)*E\n",
    "    \n",
    "    def train(self, input_data, targets):\n",
    "        for _ in range(self._epoch):\n",
    "            X = input_data.T               # (3, 4)\n",
    "            H = np.dot(self._w_input.T, X) # W.T * X : W(1, 3) X(3, 4) ==> (1, 4)\n",
    "            Y = self._sigmoid(H.T)         # Y (4,1)\n",
    "            E = targets - Y                # error  (4,1)\n",
    "            #print(self._delta(Y, E).shape,self._delta(Y, E) )\n",
    "            delta_w = self._learning_rate*self._delta(Y, E)*X.T\n",
    "            #print(delta_w.shape)\n",
    "            avg_delta_w = np.mean(delta_w, axis=0) #(1, 3)\n",
    "            #print(self._w_input)\n",
    "            self._w_input += avg_delta_w.reshape(-1, 1)\n",
    "            #print(self._w_input)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, input):\n",
    "        x = np.array(input).reshape([-1,1])\n",
    "        h = np.dot(self._w_input.T, x) # W.T * X\n",
    "        y = self._sigmoid(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.23_batch_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.24_batch_transpomation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.3 mini-batch 구현방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MiniBatchNN:\n",
    "    def __init__(self, input_node, output_node, \n",
    "                learning_rate, epoch):\n",
    "        self._input_node = input_node\n",
    "        self._output_node = output_node\n",
    "        self._learning_rate = learning_rate\n",
    "        self._epoch = epoch\n",
    "        self._w_input = np.random.uniform(size=(input_node, output_node))\n",
    "        pass\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def _d_sigmoid(self, x):\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "    def _delta(self, Y, E):\n",
    "        return d_sigmoid(Y)*E\n",
    "    \n",
    "    def _update_weight(self, input_data, targets):\n",
    "        X = input_data.T               # (3, 4)\n",
    "        H = np.dot(self._w_input.T, X) # W.T * X : W(1, 3) X(3, 4) ==> (1, 4)\n",
    "        Y = self._sigmoid(H.T)         # Y (4,1)\n",
    "        E = targets - Y                # error  (4,1)\n",
    "        #print(self._delta(Y, E).shape,self._delta(Y, E) )\n",
    "        delta_w = self._learning_rate*self._delta(Y, E)*X.T\n",
    "        #print(delta_w.shape)\n",
    "        avg_delta_w = np.mean(delta_w, axis=0) #(1, 3)\n",
    "        #print(self._w_input)\n",
    "        self._w_input += avg_delta_w.reshape(-1, 1)\n",
    "        #print(self._w_input)\n",
    "        pass\n",
    "\n",
    "\n",
    "    def train(self, input_data, targets, mini_batch):\n",
    "        for _ in range(self._epoch):\n",
    "            splited_data = np.array_split(input_data, int(len(input_data)/mini_batch))\n",
    "            spiited_targets = np.array_split(targets, int(len(targets)/mini_batch))\n",
    "            for arr_data, arr_target in zip(splited_data, spiited_targets):\n",
    "                self._update_weight(arr_data, arr_target)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, input):\n",
    "        x = np.array(input).reshape([-1,1])\n",
    "        h = np.dot(self._w_input.T, x) # W.T * X\n",
    "        y = self._sigmoid(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02979768]]\n",
      "[[ 0.01973486]]\n",
      "[[ 0.98352295]]\n",
      "[[ 0.97507906]]\n"
     ]
    }
   ],
   "source": [
    "## training data\n",
    "training_set = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 0, 1, 1],\n",
    "    [1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "input_data = training_set[:, :3]\n",
    "targets = training_set[:, [-1]]\n",
    "\n",
    "n = MiniBatchNN(3, 1, 0.05, 10000)\n",
    "#n = BatchNN(3, 1, 0.05, 1)\n",
    "n.train(input_data, targets, 2)\n",
    "\n",
    "print(n.predict([0,0,1]))\n",
    "print(n.predict([0,1,1]))\n",
    "print(n.predict([1,0,1]))\n",
    "print(n.predict([1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tenforflow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-292-19950f12f385>:27: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "2.8106\n",
      "[[  2.92065079e-06]\n",
      " [  2.92062691e-06]\n",
      " [  9.99997079e-01]\n",
      " [  9.99997079e-01]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(777) \n",
    "\n",
    "training_set = np.array([\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 1., 1., 0.],\n",
    "    [1., 0., 1., 1.],\n",
    "    [1., 1., 1., 1.]\n",
    "])\n",
    "\n",
    "x_data = training_set[:, :3]\n",
    "y_data = training_set[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "input_count = 3\n",
    "output_count = 1\n",
    "W = tf.Variable(tf.random_normal([input_count, output_count]))\n",
    "\n",
    "hypothesis = tf.matmul(X, W)\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for epoch in range(10000):\n",
    "        _, c = sess.run([train, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        if epoch % 100000 == 0 :\n",
    "            print(c)\n",
    "    print(sess.run(hypothesis, feed_dict={X:x_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.26.tensorflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy vs Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 단층 신경망의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단층이 한계는 다층으로 극복 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xor 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50165401]]\n",
      "[[ 0.50018098]]\n",
      "[[ 0.49870795]]\n",
      "[[ 0.49723494]]\n"
     ]
    }
   ],
   "source": [
    "## training data\n",
    "training_set = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 1, 1],\n",
    "    [1, 0, 1, 1],\n",
    "    [1, 1, 1, 0]\n",
    "])\n",
    "\n",
    "input_data = training_set[:, :3]\n",
    "targets = training_set[:, [-1]]\n",
    "\n",
    "n = SGDNN(3, 1, 0.05, 100000)\n",
    "n.train(input_data, targets)\n",
    "\n",
    "print(n.predict([0,0,1]))\n",
    "print(n.predict([0,1,1]))\n",
    "print(n.predict([1,0,1]))\n",
    "print(n.predict([1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률적으로 판별 불가\n",
    "  - 선형 분리 불가능\n",
    "  - linearly inseparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/02.25.xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단층 신경망은 선형 분리 가느한 문제만 적용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\varphi(v) = \\varphi(Wx+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
